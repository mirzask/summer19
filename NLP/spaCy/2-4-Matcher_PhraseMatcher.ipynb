{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based systems\n",
    "\n",
    "**Statistical models**: Statistical models are useful if your application needs to be able to generalize based on a few examples. For instance, detecting product or person names usually benefits from a statistical model. Instead of providing a list of all person names ever, your application will be able to predict whether a span of tokens is a person name. Similarly, you can predict dependency labels to find subject/object relationships.\n",
    "- Use cases: spaCy's entity recognizer, dependency parser or part-of-speech tagger\n",
    "\n",
    "**Rule-based systems**: Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find, e.g. drug names, country names.\n",
    "- In spaCy: custom tokenization rules (tokenizer), `Matcher`, `PhraseMatcher`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher\n",
    "\n",
    "- You can add patterns to the \"Vocab\" using the `matcher.add()` method.\n",
    "- Use operators (`'OP'`) to specify how often to match a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{'LEMMA': 'love', 'POS': 'VERB'}, {'LOWER': 'cats'}]\n",
    "matcher.add('LOVE_CATS', None, pattern)\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "## `+` operator matches 1 or more times\n",
    "pattern = [{'TEXT': 'very', 'OP': '+'}, {'TEXT': 'happy'}]\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9137535031263442622, 1, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is the `(ID, start_index, end_index)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print('Previous token:', doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhraseMatcher\n",
    "\n",
    "Use `PhraseMatcher` to find sequences of words. It performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.\n",
    "\n",
    "- More efficient and faster than the `Matcher`\n",
    "- Great for matching large word lists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add('DOG', None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad', '-', 'free', 'viewing']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in nlp(\"ad-free viewing\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitch      ADJ       compound  \n",
      "Prime       PROPN     nsubj     \n",
      ",           PUNCT     punct     \n",
      "the         DET       det       \n",
      "perks       NOUN      compound  \n",
      "program     NOUN      appos     \n",
      "for         ADP       prep      \n",
      "Amazon      PROPN     compound  \n",
      "Prime       PROPN     compound  \n",
      "members     NOUN      nsubj     \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"\"\"Twitch Prime, the perks program for Amazon Prime members offering free loot, games and other benefits, \n",
    "          is ditching one of its best features: ad-free viewing. According to an email sent out to Amazon Prime members\n",
    "          today, ad-free viewing will no longer be included as a part of Twitch Prime for new members, beginning on\n",
    "          September 14. However, members with existing annual subscriptions will be able to continue to enjoy ad-free\n",
    "          viewing until their subscription comes up for renewal. Those with monthly subscriptions will have access to\n",
    "          ad-free viewing until October 15.\"\"\")\n",
    "\n",
    "for token in doc[:10]:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(\"{:<12}{:<10}{:<10}\".format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Match the following:**\n",
    "\n",
    "1. \"Amazon\" plus a title-cased proper noun\n",
    "2. case-insensitive mentions of \"ad-free\", plus the following noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{'LOWER': 'ad'}, {'TEXT': '-'}, {'LOWER': 'free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', None, pattern1)\n",
    "matcher.add('PATTERN2', None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhraseMatcher Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aruba', 'Afghanistan', 'Angola', 'Anguilla', 'Ã…land Islands', 'Albania']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycountry\n",
    "\n",
    "[country.name for country in pycountry.countries][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = [country.name for country in pycountry.countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add('COUNTRY', None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czechia, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Czechia may help Slovakia protect its airspace\")\n",
    "\n",
    "# Call the matcher on the doc and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhraseMatcher to add \"GPE\" label to country matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senegal --> Senegal\n",
      "Nigeria --> Nigeria\n",
      "Egypt --> Egypt\n",
      "Tunisia --> Tunisia\n",
      "Senegal --> Senegal\n",
      "Madagascar --> Madagascar\n",
      "Egypt --> Egypt\n",
      "Benin --> Benin\n",
      "Morocco --> Morocco\n",
      "Morocco --> Morocco\n",
      "Egypt --> Egypt\n",
      "Ghana --> Ghana\n",
      "[('Senegal', 'GPE'), ('Nigeria', 'GPE'), ('Egypt', 'GPE'), ('Tunisia', 'GPE'), ('Senegal', 'GPE'), ('Madagascar', 'GPE'), ('Egypt', 'GPE'), ('Benin', 'GPE'), ('Morocco', 'GPE'), ('Morocco', 'GPE'), ('Egypt', 'GPE'), ('Ghana', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(\"\"\"Senegal retained top spot on the continent, moving up two places to reach 20th in the world - their best ever ranking.\n",
    "\n",
    "Nigeria, who won bronze in Egypt, went up 12 places to 33 on the global list and third in Africa.\n",
    "\n",
    "Tunisia, the other semi-finalists at the Nations Cup, were second in Africa, behind Senegal, but moved down four places to 29th in the world.\n",
    "\n",
    "Surprise quarter-finalists Madagascar were rewarded for their impressive run in Egypt, moving up 12 places to 96th overall.\n",
    "\n",
    "Benin - who knocked out Morocco in the last-16 - went up six places to 82nd in the world with Morocco also going up six places to 41st in the world and fifth in Africa.\n",
    "\n",
    "Nations Cup hosts Egypt went up nine spots to make the top 50, moving up to 49th overall.\n",
    "\n",
    "Ghana are just below the Pharaohs in 7th on the African list having maintained their position of 50th in the world.\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
