{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Tips\n",
    "\n",
    "You're going to feel inclined to use a more list comprehension-like form to process text, but fight the urge! Use `nlp.pipe` instead.\n",
    "\n",
    "**BAD**\n",
    "\n",
    "```\n",
    "docs = [nlp(text) for text in LOTS_OF_TEXTS]\n",
    "```\n",
    "\n",
    "**GOOD**\n",
    "\n",
    "```\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXTS))\n",
    "```\n",
    "\n",
    "> tl;dr - use `nlp.pipe` to process lists of texts. Wrap it in `list` to create a list of these docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tuple of text and metadata using Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If getting \"context\" is important, include the `as_tuples=True` argument for `nlp.pipe`. This is useful for passing in additional metadata, like an ID associated with the text, or a page number.\n",
    "\n",
    "Check out the example to clear this up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to add metadata/context as custom attributes\n",
    "\n",
    "The below example creates two extensions, \"id\" and \"page number\", which default to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension('id', default=None)\n",
    "Doc.set_extension('page_number', default=None)\n",
    "\n",
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context['id']\n",
    "    doc._.page_number = context['page_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use *only* the tokenizer\n",
    "\n",
    "If you only need to tokenize the doc, then doing the whole nlp pipeline, i.e. POS, dependency, NER, eats up unnecessary time/resources.\n",
    "\n",
    "**Solution**: If you only need a tokenized `Doc` object, you can use the `nlp.make_doc` method instead, which takes a text and returns a Doc.\n",
    "\n",
    "**BAD**\n",
    "\n",
    "```\n",
    "doc = nlp(\"Hello world\")\n",
    "```\n",
    "\n",
    "**GOOD**\n",
    "\n",
    "```\n",
    "doc = nlp.make_doc(\"Hello world!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable components of a pipeline\n",
    "\n",
    "Similar to above, if you want to turn off the \"tagger\" for instance, you can use `nlp.disable_pipes` to do just that.\n",
    "\n",
    "Example:\n",
    "```\n",
    "# Disable tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "# Example - Disable the tagger and parser\n",
    "# i.e. should be left w/ tokenize -> NER\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Examples\n",
    "\n",
    "## Example - Process bunch of tweets\n",
    "\n",
    "Create a pipeline to process the tweets and extract the **adjectives**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "[]\n",
      "['terrible', 'gettin', 'payin']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "TEXTS = ['McDonalds is my favorite restaurant.', \n",
    "         'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..', \n",
    "         'People really still eat McDonalds :(', 'The McDonalds in Spain has chicken wings. My heart is so happy ', \n",
    "         '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P', \n",
    "         'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D', \n",
    "         'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']\n",
    "\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE **WRONG** APPROACH\n",
    "\n",
    "# for text in TEXTS:\n",
    "#     doc = nlp(text)\n",
    "#     print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the entities from the tweets.**\n",
    "\n",
    "HINT: use `list(nlp.pipe(TEXT))` for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) (@McDonalds,) (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) (WANT, McRib) (This morning,)\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Add author, book metadata to quotes\n",
    "\n",
    "Here, the text data is available as a list of tuples `[text, context]`, where `context` is often in the form of a dictionary, e.g. `{'author': 'Frank Harrell', 'book': 'Regression Modeling Strategies'}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "req = requests.get(\"https://raw.githubusercontent.com/ines/spacy-course/master/exercises/bookquotes.json\")\n",
    "DATA = [item for item in req.json()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.',\n",
       "  {'author': 'Franz Kafka', 'book': 'Metamorphosis'}],\n",
       " [\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
       "  {'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'}],\n",
       " ['It was the best of times, it was the worst of times.',\n",
       "  {'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'}],\n",
       " ['The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.',\n",
       "  {'author': 'Jack Kerouac', 'book': 'On the Road'}],\n",
       " ['It was a bright cold day in April, and the clocks were striking thirteen.',\n",
       "  {'author': 'George Orwell', 'book': '1984'}],\n",
       " ['Nowadays people know the price of everything and the value of nothing.',\n",
       "  {'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Doc extension 'author' (default None)\n",
    "Doc.set_extension('author', default=None)\n",
    "\n",
    "# Register the Doc extension 'book' (default None)\n",
    "Doc.set_extension('book', default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, \"\\n\", \"— '{}' by {}\".format(doc._.book, doc._.author), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
