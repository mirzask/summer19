{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW and TF-IDF with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common terms per document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Abilify 30 mg tablet\t1 (one) tablet by mouth daily\n",
    "Accu-Chek Aviva Plus test strips\t4 (four) test strips miscellaneous daily as needed for E11.65\n",
    "Accu-Chek FastClix\t1 (one) lancet miscellaneous three times a day\n",
    "Accu-Chek Multiclix Lancet\t4 (four) lancets miscellaneous daily for ninety day(s) as needed for E11.65\n",
    "Accu-Chek Nano\t1 (one) meter kit miscellaneous one time only\n",
    "Accu-Chek SmartView Test Strips\t3 (three) test strips miscellaneous daily\n",
    "acetaminophen 300 mg-codeine 30 mg tablet (Also Known As Tylenol-Codeine #3)\t1 (one) tablet by mouth three times a day as needed for pain\n",
    "acetaminophen 325 mg tablet\t1 (one) tablet by mouth every 4 hours for ten days as needed for pain\n",
    "acetaminophen 325 mg tablet\t2 (two) tablets by mouth every 8 hours for fourteen days\n",
    "acetaminophen 500 mg tablet\t2 (two) tablets by mouth every 8 hours for seven days\n",
    "acetaminophen 500 mg tablet\t2 (two) tablets by mouth every 8 hours for two days as needed for pain\n",
    "acetazolamide 250 mg tablet\t1 (one) tablet by mouth three times a day\n",
    "acyclovir 400 mg tablet (Also Known As Zovirax)\t1 (one) tablet by mouth daily\n",
    "acyclovir 400 mg tablet (Also Known As Zovirax)\t1 (one) tablet by mouth twice a day\n",
    "acyclovir 400 mg tablet (Also Known As Zovirax)\t1 (one) tablet by mouth twice a day for fourteen days\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [word_tokenize(doc.lower()) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['abilify',\n",
       "  '30',\n",
       "  'mg',\n",
       "  'tablet',\n",
       "  '1',\n",
       "  '(',\n",
       "  'one',\n",
       "  ')',\n",
       "  'tablet',\n",
       "  'by',\n",
       "  'mouth',\n",
       "  'daily']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a gensim dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = Dictionary(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the token id in the dictionary for Acetaminophen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict.token2id.get(\"acetaminophen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a gensim corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dict.doc2bow(doc) for doc in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 2)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words with `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the 5th document in our corpus by word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (1, 2),\n",
       " (16, 2),\n",
       " (6, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (15, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (24, 1),\n",
       " (26, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = sorted(corpus[4], key = lambda w:w[1], reverse=True)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top 5 words of the 5th document w/ the count of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 2\n",
      ") 2\n",
      "for 2\n",
      "daily 1\n",
      "4 1\n",
      "accu-chek 1\n"
     ]
    }
   ],
   "source": [
    "for word_id, word_count in bow[:6]:\n",
    "    print(dict.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times does each word appear in our *entire* corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the total_word_count w/ most common words first\n",
    "sorted_word_count = sorted(total_word_count.items(), key = lambda w:w[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 20\n",
      ") 20\n",
      "tablet 17\n",
      "for 11\n",
      "by 10\n",
      "mg 10\n"
     ]
    }
   ],
   "source": [
    "for word_id, word_count in sorted_word_count[:6]:\n",
    "    print(dict.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Determine which words in the corpus are important. Downweight less important words, e.g. common words such as mg, the, by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.030431918490774385),\n",
       " (1, 0.030431918490774385),\n",
       " (2, 0.15463304814088327),\n",
       " (3, 0.5203314451468743),\n",
       " (4, 0.6888618767345641),\n",
       " (5, 0.12901590119992506),\n",
       " (6, 0.2975463327876148),\n",
       " (7, 0.12901590119992506),\n",
       " (8, 0.12901590119992506),\n",
       " (9, 0.15463304814088327),\n",
       " (10, 0.25803180239985013)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0.030431918490774385\n",
      ") 0.030431918490774385\n",
      "1 0.15463304814088327\n",
      "30 0.5203314451468743\n",
      "abilify 0.6888618767345641\n",
      "by 0.12901590119992506\n",
      "daily 0.2975463327876148\n",
      "mg 0.12901590119992506\n",
      "mouth 0.12901590119992506\n",
      "one 0.15463304814088327\n",
      "tablet 0.25803180239985013\n"
     ]
    }
   ],
   "source": [
    "for word_id, score in tfidf[corpus[1]]:\n",
    "    print(dict.get(word_id), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would expect, \"abilify\" being the more unique of the words in this script has the highest tf-idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
