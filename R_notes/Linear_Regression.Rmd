---
title: "Linear Regression"
author: "Mirza S. Khan"
date: "7/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(data.table, tidyverse, h2o, caret, inspectdf, broom, kableExtra)

bike <- fread("https://raw.githubusercontent.com/christophM/interpretable-ml-book/master/data/bike-sharing-daily.csv")

bike$weekday = factor(bike$weekday, levels=0:6, labels = c('SUN', 'MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT'))
  bike$holiday = factor(bike$holiday, levels = c(0,1), labels = c('NO HOLIDAY', 'HOLIDAY'))
  bike$workingday = factor(bike$workingday, levels = c(0,1), labels = c('NO WORKING DAY', 'WORKING DAY'))
  bike$season = factor(bike$season, levels = 1:4, labels = c('SPRING', 'SUMMER', 'FALL', 'WINTER'))
  bike$weathersit = factor(bike$weathersit, levels = 1:3, labels = c('GOOD', 'MISTY', 'RAIN/SNOW/STORM'))
  bike$mnth = factor(bike$mnth, levels = 1:12, labels = c('JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OKT', 'NOV', 'DEZ'))
  bike$yr[bike$yr == 0] = 2011
  bike$yr[bike$yr == 1] = 2012
  bike$yr = factor(bike$yr)
  bike$days_since_2011 = day_diff(bike$dteday, min(as.Date(bike$dteday)))

  # denormalize weather features:
  # temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)
  bike$temp = bike$temp * (39 - (-8)) + (-8)
  # atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)
  bike$atemp = bike$atemp * (50 - (16)) + (16)

  #windspeed: Normalized wind speed. The values are divided to 67 (max)
  bike$windspeed = 67 * bike$windspeed
  #hum: Normalized humidity. The values are divided to 100 (max)
  bike$hum = 100 * bike$hum
  
bike <- bike %>% 
  mutate(days_since_2011 = difftime(dteday, min(as.Date(dteday)), units = "days"))


theme_set(theme_minimal())
```

```{r}
inspect_types(bike) %>% 
  show_plot()

inspect_na(bike)

inspect_num(bike)

inspect_num(bike) %>% 
  show_plot()

inspect_imb(bike)

inspect_imb(bike) %>% 
  show_plot()

inspect_cat(bike) %>% 
  show_plot()

bike %>% 
  inspect_cor() %>% 
  show_plot()
```


```{r}
biker <- bike %>% 
  select(c('cnt', 'season','holiday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'days_since_2011')) %>% 
  as.data.table()
```

```{r}
model <- biker %>% 
  lm(cnt ~ ., data = .)

model %>%
  tidy(conf.int = TRUE)
```

## Weight Plot

```{r}
model %>%
  tidy(conf.int = TRUE) %>% 
  # get rid of intercept term
  filter(term != "(Intercept)") %>% 
  # Clean up the names a bit
  mutate(term = str_replace(term, "workingday", ""),
         term = str_replace(term, "weathersit", "Weather: "),
         term = str_replace(term, "season", "Season: "),
         term = str_replace(term, "holiday", ""),
         term = str_to_title(term),
         term = fct_reorder(term, estimate)
         ) %>% 
  ggplot(aes(estimate, term)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "grey")
```

Downside: features are measured on different scales. Solution = normalization prior to fitting linear model (e.g. centering and scaling -> mean 0 and s.d. 1) _or_ Effect plot.

## ANOVA

```{r}
anova(model) %>% 
  tidy() %>% 
  mutate(sumsq / sum(sumsq)) %>% 
  kable()
```

```{r}
model %>%
  augment(data = biker) %>% 
  ggplot(aes(.fitted, cnt)) +
  geom_point(alpha = .1) +
  geom_smooth(method = "lm")
```


## Effect Plot

Multiply weight from linear regression model by the actual feature values.

$$
\operatorname{effect}_{j}^{(i)}=w_{j} x_{j}^{(i)}
$$

```{r}
#TODO
```


## LASSO

```{r}
#parsnip
library(recipes)
library(parsnip)

prep_bike <- recipe(cnt ~ ., data = biker) %>% 
  step_normalize(all_numeric()) %>% 
  prep() %>% 
  juice()

prep_bike

mod_p <- linear_reg(mode = "regression", mixture = 1, penalty = 0.1) %>% 
  set_engine("glmnet") %>% 
  fit(cnt ~ ., data = prep_bike)

mod_p %>% 
  tidy()
```

Erin Ledell's tips on debugging things to get H2O working are [here](https://twitter.com/ledell/status/1148512123083010048).

```{r}
#h2o
# family = "gaussian"

h2o.init()

biker.hex <- as.h2o(biker)

h2o.glm(y = "cnt", 
        training_frame = biker,
        family = "gaussian",
        lambda = 0,
        alpha = 1,
        lambda_search = FALSE,
        nfolds = 0,
        remove_collinear_columns = TRUE,
        compute_p_values = TRUE)
```


```{r}
# caret


tr_ctrl <- trainControl(method = "none",
                        savePredictions = "final")

# ctrl <- trainControl(method = "cv",
#                      number = 10,
#                      savePredictions = "final")

## alpha = 1 is lasso, alpha = 0 is ridge
lasso_grid <- expand.grid(alpha = 1,
                          lambda = 0.1)

# for some reason the preProc method doesn't seem to be acting right over here
mod_c <- train(cnt ~ .,
               data = biker,
               method = "glmnet",
               preProc = c("center", "scale"),
               trControl = tr_ctrl,
               tuneGrid = lasso_grid)

coef(mod_c$finalModel, mod_c$bestTune$lambda) %>% 
  tidy()
coef(mod_c$finalModel, mod_c$bestTune$lambda)
```

